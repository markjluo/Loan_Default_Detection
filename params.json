{"name":"Boosting credit default detection: Predict how capable applicants are of repaying loans by Gradient Boosting Method","tagline":"- Jiaqi Jiang, Jiatuan Luo, Jingyu Li, Jingyu Zou, Shuyu Ding","body":"\r\n## 1. Introduction\r\n\r\nPredicting default risk is an essential problem for financial institutions. Leveraging machine learning techniques with business data to make good predictions will not only empower trustworthy clients to be successful by providing loans, but also help financial institutions control business risk.\r\n\r\nOur project focuses on using supervised and unsupervised machine learning techniques to predict applicant’s repayment ability based on historical loan application data and a wide range of credit and transactional data. Our project uses the “Home Credit Default Risk” dataset from Kaggle (300k+ applicants and 200+ features). The dataset is provided by Home Credit, a company that provides loans to unbanked populations who have insufficient credit histories. \r\n\r\n## 2. Literature Review\r\n\r\nFrom the machine learning perspective, our project is a binary classification problem, in which we need to predict whether the applicant can repay the loan. Traditional solutions of binary classification problems like Logistic Regression, SVM, Decision Tree and Naive Bayes could be implemented to build a model directly from the data. Besides building one single “strong” model, the other approach would be to build an ensemble of models to further get better prediction. \r\n\r\nThe main hypothesis behind ensemble learning approach is that when weak models are correctly combined, we can get more accurate models. Ensemble learning contains two approaches: Bagging and Boosting [1]. Bagging is a parallel process in which a large number of sub samples are generated by bootstrap sampling and then each weak model is trained on the sub sample independently. The final prediction is generated by ensembling the predictions from all the weak models (e.g. voting). Random Forest is a typical bagging method. Different with Bagging, Boosting is a sequential and additive process in which each weak model is learned iteratively aiming at decreasing the bias of previous weak models. The new weak model is learned based on the former model so that it cannot be run in a parallel manner. The first successful boosting algorithm is the Adaboost algorithm. It uses the error rate of each weak model to update the sampling weights of data points and ensemble weights of weak models so that misclassified instances will be more likely sampled in the latter rounds and weak models with lower error rate will get higher weights in the final ensemble process. The drawback of Adaboosting is that it’s sensitive to outliers and there’s no flexibility for different loss functions. Then, Gradient Boosting Machine (GBM) was proposed by Friedma, which is more robust and adaptive to different problems compared with Adaboost [2].\r\n\r\nGradient Boosting adopted the concept of gradient descend, which is used to update the parameters to minimize loss function of a model. As an additive model, the m-th weak model can be described as:\r\n\r\n\r\n![image](images/formula1.png)         (1)\r\n\r\nThe objective loss function in this step can be expressed as following, in which we need find a model f that can minimize the function.\r\n\r\n![image](images/formula2.png)        (2)\r\n\r\nThus, similar as gradient descend, we can regard the whole function f as the parameter and get the gradient descend equation below.\r\n\r\n![image](images/formual3%20.png)       (3)\r\n\r\nComparing equation (1) and (3), we can get the core process in Gradient Boosting: \r\n\r\n- Step 1: get the steepest negative descent by calculating equation (4). The value diis often called pseudo residual.\r\n![image](images/formula4.png)       (4)\r\n    \r\n- Step 2: find ![image](images/hmx.png) to fit the descent by minimizing the errors between ![image](images/render.png) and ![image](images/hmx.png). \r\n\r\n- Step 3: After we get  ![image](images/hmx.png), we plug equation (1) into function (2) and we can obtain the learning rate mby minimizing the loss function (2).\r\n\r\n- Step 4: plug in  ![image](images/hmx.png)from step two and mfrom step three into equation (1) to get the new weak learner  ![image](images/fmx.png)\r\n\r\nRegarding Gradient Boosting Machine’s application of classification problems, many research and practical projects in different domains showed its excellent predicting power. GBM can also simultaneously accomplish feature selection, which can be well leveraged in real business problems. What’s more, Brown and Mues [3] demonstrated that with the pronounced class imbalanced dataset, gradient boosting classifiers performed significantly better than traditional methods like KNN and decision tree algorithms. Considering loan default classification problem is a typical scenario that the distribution of default and non-default cases is imbalanced, we adopted GBM in our project. LightGBM is a gradient boosting tree framework created by Microsoft, which is highly efficient and scalable. The researchers experimented on multiple public datasets and showed that LightGBM speeds up the training process compared with conventional Gradient Boosting Decision Tree without significantly lossing predicting accuracy [4].\r\n\r\nTo summarize, our goal of this project is to experiment with the classical approach of classification (as baseline models), as well as Gradient Boosting Machine to classify loan default. We aim to improve the ROC_AUC score from our baseline models by using lightGBM framework.\r\n\r\n\r\n## 3. Dataset Description and Analysis\r\n### 3.1 Dataset Description\r\n\r\nOur project used the “Home Credit Default Risk” dataset from Kaggle, which was provided by Home Credit. Besides the main dataset including the information of current applications, Home Credit also provided several datasets including applicants’ previous credit and transaction history. The detailed description of each dataset and the way they are related is shown below.\r\n\r\n![image](images/7641%20project%20plot/dataset_relationship.png)\r\n\r\nFigure 1.  The Description and Relationship of All Dataset\r\n\r\n### 3.2 Data preparation\r\n\r\nThe data pre-processing process includes outlier detection, missing value imputation, categorical variable encoding, data aggregation and datasets join. Since there are seven datasets we need to process, we set some uniform rules for data pre-processing.\r\n#### 3.2.1 Outlier Detection\r\n\r\nWe only defined the infeasible values in each feature as outliers. In the datasets, all the features related to time length is defined in days using the starting date minus the ending date. For example, “DAYS_BIRTH” is a feature representing the client's age in days which is calculated by using birth date minus application date. As a result, the value of all these features should be negative. We found a number of positive values in these features so that we defined them as outliers and regarded them as missing value (e.g. NaN). There are no other obvious outliers in the datasets.\r\n\r\n#### 3.2.2 Missing Value Imputation\r\n\r\nAs real business datasets, missing values exist in a large proportion of features (as shown in Table 1). In order to retain as much information as possible, we didn’t remove any features in this step. We applied different ways to conduct missing value imputation. For the categorical variables, if the percentage of missing values is below 5%, we used the mode of that feature to impute the missing values; otherwise, we added a new category called “ismissing” in the feature. For the numerical variables, if the percentage of missing values is below 5%, we used the median of that feature to impute the missing values; otherwise, we didn’t impute any numbers in this step.\r\n\r\nTabel 1. Missing Value Stats of Each Dataset\r\n<table class=\"tg\">\r\n  <tr>\r\n    <th class=\"tg-0pky\" rowspan=\"2\">Dataset </th>\r\n    <th class=\"tg-0pky\" rowspan=\"2\">Total columns(exclude response column)</th>\r\n    <th class=\"tg-0pky\" colspan=\"5\"># of Columns with different missing value percentage</th>\r\n    <th class=\"tg-0lax\" rowspan=\"2\">Max missing value percentage</th>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\">0</td>\r\n    <td class=\"tg-0pky\">(0, 5%]</td>\r\n    <td class=\"tg-0pky\">(5%, 20%]</td>\r\n    <td class=\"tg-0lax\">(20%, 50%</td>\r\n    <td class=\"tg-0pky\">(50%, 100%]</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\">application_train.csv</td>\r\n    <td class=\"tg-0pky\">121</td>\r\n    <td class=\"tg-0pky\">54</td>\r\n    <td class=\"tg-0pky\">10</td>\r\n    <td class=\"tg-0pky\">7</td>\r\n    <td class=\"tg-0lax\">9</td>\r\n    <td class=\"tg-0pky\">41</td>\r\n    <td class=\"tg-0lax\">69.9%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\">bureau.csv</td>\r\n    <td class=\"tg-0pky\">17</td>\r\n    <td class=\"tg-0pky\">10</td>\r\n    <td class=\"tg-0pky\">1</td>\r\n    <td class=\"tg-0pky\">2</td>\r\n    <td class=\"tg-0lax\">2</td>\r\n    <td class=\"tg-0pky\">2</td>\r\n    <td class=\"tg-0lax\">71.5%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">bureau_balance.csv</td>\r\n    <td class=\"tg-0lax\">3</td>\r\n    <td class=\"tg-0lax\">3</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0.0%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">previous_application.csv</td>\r\n    <td class=\"tg-0lax\">37</td>\r\n    <td class=\"tg-0lax\">21</td>\r\n    <td class=\"tg-0lax\">2</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">10</td>\r\n    <td class=\"tg-0lax\">4</td>\r\n    <td class=\"tg-0lax\">99.6%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">POS_CASH_balance.csv</td>\r\n    <td class=\"tg-0lax\">8</td>\r\n    <td class=\"tg-0lax\">6</td>\r\n    <td class=\"tg-0lax\">2</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0.3%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">installments_payments.csv</td>\r\n    <td class=\"tg-0lax\">8</td>\r\n    <td class=\"tg-0lax\">6</td>\r\n    <td class=\"tg-0lax\">2</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0.02%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">credit_card_balance.csv</td>\r\n    <td class=\"tg-0lax\">23</td>\r\n    <td class=\"tg-0lax\">14</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">9</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">0</td>\r\n    <td class=\"tg-0lax\">20.0%</td>\r\n  </tr>\r\n</table>\r\n\r\n#### 3.2.3 Categorical Variable Encoding\r\n\r\nWe applied one hot encoding for all categorical variables so that the binary variables corresponding to each categorical variable were created and the original categorical variable was removed.\r\n\r\n#### 3.2.4 Data Aggregation\r\n\r\nAs described in section 3.1, for each loan application in the main table, several rows of records of previous credit history may exist in the rest of the datasets. So for each dataset except the main table, we aggregated the records which were related to the same applicant in current loan applications. We used a greedy way to create the aggregated features for each loan application. In detail, for most of the continuous variables, we aggregated them by calculating min(), max() and mean(). For some special variables, we even recorded aggregated features by using sum() and count(), which was decided according to the definition and property of the feature. On the other hand, for the binary variables, we aggregated them by calculating mean(), which in fact was a representation of what percentage a certain category label appeared in history records for a certain applicant. Detailed aggregation method can be referred to Appendix.\r\n\r\nIn this data aggregation step, we ignored missing values when aggregating. We proposed that since we didn’t choose to remove any columns in this step, using mean value to impute missing values is another widely-used method. In our project, we thought aggregating each column by using mean() is equivalent to imputing missing values by using mean values and then conducting aggregation. \r\n\r\n#### 3.2.5 Datasets join\r\n\r\nThe last step of data pre-processing is joining all the dataset by the ID keys. All other six datasets were joined to the main table “application_train.csv”. The following figure shows how we joined the datasets together. The size of the final dataset we got was (307,511, 754). Excluding the ID column (“SK_ID_CURR”) and response column (“TARGET”), we had 752 features in our dataset. \r\n\r\n![image](images/7641%20project%20plot/data_agg_join.png)\r\n\r\nFigure 2.  Schematic Plot of Dataset Aggregation and Join\r\n\r\n### 3.3 Data Exploration and Feature Engineering\r\n#### 3.3.1 Response Variable\r\n\r\nAs a classification problem, the response variable in our dataset is binary (1=default, 0=not default). We noticed that the distribution of two classes was imbalanced. The number of default cases in the dataset is 24,825, 8.1% of the total sample size. The imbalanced data influenced our modeling approach, which we discussed in the latter sections.\r\n\r\n![image](images/7641%20project%20plot/imbalanced_label.png)\r\n\r\nFigure 3.  Number of Different Labels in Dataset\r\n\r\n#### 3.3.2 Creating Features by Domain Knowledge\r\n\r\nAlthough the original datasets already contained a large number of features, we created several new features based on our financial knowledge, life experience and knowledge shared on Kaggle’s discussion board. Table 2 below shows the new feature we created. The main logic that drove our feature engineering was that we tried to create features that may related with the client’s capability of repaying the loans, compared with the amount of loans he got.\r\n\r\n Table 2.  New Features Created\r\n\r\n<table class=\"tg\">\r\n  <tr>\r\n    <th class=\"tg-0pky\">Dataset </th>\r\n    <th class=\"tg-0pky\">Feature</th>\r\n    <th class=\"tg-0lax\">Description</th>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\" rowspan=\"5\">application_train.csv</td>\r\n    <td class=\"tg-0pky\">CREDIT_INCOME_PERCENT</td>\r\n    <td class=\"tg-0lax\">the percentage of the credit amount relative to a client's income</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\">ANNUITY_INCOME_PERCENT</td>\r\n    <td class=\"tg-0lax\">the percentage of the loan annuity relative to a client's income</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">CREDIT_TERM</td>\r\n    <td class=\"tg-0lax\">the length of the payment in months since the annuity is the monthly amount due</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">INCOME_PER_PERSON</td>\r\n    <td class=\"tg-0lax\">the average income per-person in a household</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">DAYS_EMPLOYED_PERCENT</td>\r\n    <td class=\"tg-0lax\">the percentage of the days employed relative to the client's age</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">previous_application.csv</td>\r\n    <td class=\"tg-0lax\">AMT_ACCEPT_RATE</td>\r\n    <td class=\"tg-0lax\">credit offered compared to the amount the client applied</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\" rowspan=\"3\">installments_payments.csv</td>\r\n    <td class=\"tg-0lax\">PAYMENT_PERCENT</td>\r\n    <td class=\"tg-0lax\">the amount that client actually paid compared to the amount of installment</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PAYMENT_GAP</td>\r\n    <td class=\"tg-0lax\">the gap between the amount of installment and the actual amount the client paid</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PAYMENT_TIME</td>\r\n    <td class=\"tg-0lax\">the time length between installment due date and actual paying date; positive value means the client missed the due, and negative value means the client paid earlier</td>\r\n  </tr>\r\n</table>\r\n\r\nFor these newly created features, we plotted the value distribution of two user groups (e.g TARGET = 1 and TARGET = 0). The density plot illustrated some degree of difference between the default group and non-default group, which suggested that these features may be effective in classification prediction.\r\n\r\n![image](images/7641%20project%20plot/feature_density.png)\r\n\r\nFigure 4.  Density Plot of Created Features\r\n\r\n#### 3.3.3 Using Clustering Method to Create New Features\r\n#### K-means Clustering\r\n\r\nOur project focused on applying Gradient Boosting Approach on the classification task. We proposed that using clustering methods to explore the underlying structure and patterns of the clients could be an effective way to generate new features. As Gradient Boosting is a tree-based model, the tags of different clusters could be good features in node splitting. We had a large number of features in our dataset. To segment the clients, we thought it would be more reasonable and proper to use features that are directly and explicitly related with the repaying capability.  On the other hand, we planned not to select many features in clustering analysis so that it was easier for us to evaluate and interpret the results. At last, we chose three features: “CREDIT_INCOME_PERCENT”, “ANNUITY_INCOME_PERCENT” and “DAYS_BIRTH” from current application dataset “application_train.csv”. The first two features are direct measurement on repaying capability and we thought clients’ age may be a moderate factor here, which means for two clients with similar values on “CREDIT_INCOME_PERCENT” and “ANNUITY_INCOME_PERCENT”, people’s perception on their default risk may be different according to their age.\r\n\r\nWe used the K-means method to conduct clustering analysis, since the distribution of three features didn’t show significant multi-modal property. We first standardized three features and got the z score of each feature. Then we conducted K-means clustering and tuned the number of clusters. We plotted the elbow plot and chose the number of clusters to be 8. The basic statistics of 8 clusters is illustrated in Table 3.\r\n\r\n![image](images/7641%20project%20plot/elbow_plot.png)\r\nFigure 5.  Elbow Plot of K-Means Clustering\r\n\r\nTable 3.  Basic Statistics of Clusters\r\n\r\n<table class=\"tg\">\r\n  <tr>\r\n    <th class=\"tg-0pky\" rowspan=\"2\">Cluster ID</th>\r\n    <th class=\"tg-0pky\" rowspan=\"2\">Count</th>\r\n    <th class=\"tg-0lax\" colspan=\"3\">Mean Value</th>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Age</td>\r\n    <td class=\"tg-0lax\">Credit/Income</td>\r\n    <td class=\"tg-0lax\">Annuity/Income</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\">Cluster 0</td>\r\n    <td class=\"tg-0pky\">53,549</td>\r\n    <td class=\"tg-0lax\">33.9</td>\r\n    <td class=\"tg-0lax\">405%</td>\r\n    <td class=\"tg-0lax\">20% </td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Cluster 1</td>\r\n    <td class=\"tg-0lax\">45,168</td>\r\n    <td class=\"tg-0lax\">59.0</td>\r\n    <td class=\"tg-0lax\">227%</td>\r\n    <td class=\"tg-0lax\">12%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Cluster 2</td>\r\n    <td class=\"tg-0lax\">6,515</td>\r\n    <td class=\"tg-0lax\">47.1</td>\r\n    <td class=\"tg-0lax\">1329%</td>\r\n    <td class=\"tg-0lax\">49%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Cluster 3</td>\r\n    <td class=\"tg-0lax\">48,448</td>\r\n    <td class=\"tg-0lax\">54.6</td>\r\n    <td class=\"tg-0lax\">481%</td>\r\n    <td class=\"tg-0lax\">21%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Cluster 4</td>\r\n    <td class=\"tg-0lax\">50,701</td>\r\n    <td class=\"tg-0lax\">30.0</td>\r\n    <td class=\"tg-0lax\">187%</td>\r\n    <td class=\"tg-0lax\">11%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Cluster 5</td>\r\n    <td class=\"tg-0lax\">19,962</td>\r\n    <td class=\"tg-0lax\">55.5</td>\r\n    <td class=\"tg-0lax\">820%</td>\r\n    <td class=\"tg-0lax\">31%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Cluster 6</td>\r\n    <td class=\"tg-0lax\">56,442</td>\r\n    <td class=\"tg-0lax\">43.9</td>\r\n    <td class=\"tg-0lax\">229%</td>\r\n    <td class=\"tg-0lax\">12%</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Cluster 7</td>\r\n    <td class=\"tg-0lax\">26,726</td>\r\n    <td class=\"tg-0lax\">36.1</td>\r\n    <td class=\"tg-0lax\">714%</td>\r\n    <td class=\"tg-0lax\">30%</td>\r\n  </tr>\r\n</table>\r\n\r\n#### Visualizing and Analyzing Clustering Results\r\n\r\nWe used three features in the clustering analysis and the number of data points is huge. Plotting the data points in a 3-dimensional scatter plot can not present different clusters in a clear way. We tried the T-Distributed Stochastic Neighboring Entities (t-SNE) technique to reduce the dimension and generate a 2-dimensional density plot for each cluster. The general algorithm of t-SNE is to “minimize the divergence between distribution that measures pairwise similarities of the input objects and distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding” [5]. The 2-dimensional density plot after t-SNE transformation is presented below. The density plots show some degree of divergence among clusters. However, compared with the number of samples, it’s plausible that eight clusters were not sufficient enough to split all users into clear subgroups, thus overlaps existed.\r\n\r\n![image](images/7641%20project%20plot/tsne.png)\r\n\r\nFigure 6.  Two-dimension Density Plot after t-SNE Transformation\r\n\r\nWe then analyzed the distribution of the response variable in 8 clusters. The following bar plot shows that the proportion of default cases varies among clusters, which indicates that segmenting users into subgroups and using group tags (e.g. binary indicator on whether each client belongs to a certain cluster) as the new features can provide potential explanation power in the prediction model.\r\n    \r\n![image](images/7641%20project%20plot/cluster_default_proportion.png)\r\n\r\nFigure 7.  Proportion of Default Samples in Each Cluster\r\n\r\n## 4 Experiment Settings and Baselines\r\n### 4.1 Data Preparation\r\n#### 4.1.1 Missing Value Imputation\r\n\r\nSince all the baseline models we planned to use cannot handle missing values automatically, we conducted missing value imputation in this step. Among the 752 features, 192 features have no missing value; 397 features have 5%-20% missing values; 13 features have 20%-50% missing values and 150 features have more than 50% missing values. We excluded the 163 features which have more than 20% value missing. For the remaining features with missing values, we used mode to impute binary variables and median to impute continuous variables.\r\n\r\n#### 4.1.2 PCA\r\n\r\nAfter excluding features with 20% and more missing values, we were still left with 591 features. Besides fitting the baseline models with all these original features, we also tried to conduct feature reduction, which is helpful for avoiding overfitting and multicollinearity. We applied PCA on our dataset to reduce dimensions. We preserved the most information of our dataset by retaining the first 217 principal components in the dataset which explained 99% of the variance. And we transformed our data with the new coordinate system to create a new dataset with less dimensions. \r\n\r\n#### 4.1.3 Dataset Split\r\n\r\nFollowing the typical data analytics process, we split the whole dataset as the training set (80% of the whole dataset) and the test set (remaining 20%). The training set is for model training and validation purposes. We use the training set to train each machine learning model and perform hyperparameter tuning to select the parameter settings that produce the best result. After we choose the best model, we will use the whole training set to retrain the model with the selected hyperparameters and then use the test set to evaluate the performance of the selected model.\r\n\r\n#### 4.1.4 Oversampling: SMOTE\r\n\r\nOur dataset is extremely imbalanced that 8.78% of our data has a label of 1 and remaining 91.22% has a label of 0. We also explored whether the oversampling technique can improve the performance of our models. Since traditional oversampling method results in duplicated entries which causes problems in our cross-validation results in our experiments and undersampling significantly reduces the amount of data we have at our disposal, we elected to apply SMOTE (Synthetic Minority Oversampling Technique) on our training dataset to artificially generate new entries in the proximity of the minority class. This allows us to generate a more balanced data set. \r\n\r\nWe only conducted oversampling on the training set since the distribution of two classes in the test set should maintain an imbalanced state which is a ground-truth state. We used class weight 6:4 to oversample our minority class. After oversampling, 40% of our training data has a label of 1.\r\n\r\nIn total, we created four versions of training sets for baseline model comparison. The original dataset, the dataset with reduced dimensionality via PCA, The SMOTE oversampled dataset, and the SMOTE oversampled dataset after dimensionality reduction. We will run our baseline classification models on each of the four datasets to see which produces the best result. On the other hand, lightGBM is robust for imbalanced dataset. In the meantime, it can impute missing value when fitting the model. So we didn’t perform any data cleaning described in this section for the dataset for lightGBM. \r\n\r\nTable 4.  Five Versions of Training Set\r\n\r\n<table class=\"tg\">\r\n  <tr>\r\n    <th class=\"tg-0pky\">Models</th>\r\n    <th class=\"tg-0pky\">Training set version</th>\r\n    <th class=\"tg-0lax\">Description</th>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\" rowspan='4'>Baseline Models</td>\r\n    <td class=\"tg-0pky\">X_train_origin,<br>y_train_origin</td>\r\n    <td class=\"tg-0pky\">Training set with the original 591 features</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\">X_train_pca,<br>y_train_pca</td>\r\n    <td class=\"tg-0pky\">Training set after PCA transformation, containing 217 principal components</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pkx\">X_train_origin_os,<br>y_train_origin_os</td>\r\n    <td class=\"tg-0pk\">Oversampled training set with original features</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pk\">X_train_pca_os,<br>y_train_pca_os</td>\r\n    <td class=\"tg-0pk\">Oversampled training set with principal components</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">lightGBM</td>\r\n    <td class=\"tg-0lax\">X_train_retainmissing,<br>y_train_retainmissing</td>\r\n    <td class=\"tg-0lax\">Training set with the original 752 features</td>\r\n  </tr>\r\n</table>\r\n\r\n### 4.2 Model Evaluation Process and Metrics\r\n\r\nWe use cross validation on the training set to train different models, tune hyper parameters of each model and compare the evaluation metrics of these models (including the baseline models and lightGBM) so that we can select the one with best performance. After getting the best model, we then use this selected model to make predictions on the test set to obtain the final evaluation of the model’s performance (see Figure 8.)\r\n\r\n![image](images/7641%20project%20plot/model_selection.png)\r\n\r\nFigure 8.  Model Selection and Evaluation Process\r\n\r\nWe choose AUC instead of accuracy score or F1 score as the metric to evaluate our model. Because our data is imbalanced towards non-default, accuracy scores will be very high; F1 Score which calculates as  the harmonic mean of precision and recall is a great metric to measure model performance when the data is imbalanced. However, it only measures the performance of the model at a specific threshold (for example larger than 0.5 will be classified as 1). However, different credit issuers have different risk tolerance, thus different thresholds, in this setting ROC_AUC which measures the performance of all thresholds should be a better performance metric.\r\n\r\nF1 score is calculated using precision (TruePositives / (TruePositives + FalsePositives)) and recall (TruePositives / (TruePositives + FalseNegatives)). Because TruePositives are ususally small in the imbalanced dataset, precision is very sensitive to FalsePositives. In comparison ROC_AUC score are measured using 1-specifity (FalsePositives / (FalsePositives+TrueNegatives)) and recall (TruePositives/ (TruePositives + FalseNegatives)), because TrueNegatives is usually large, 1-speicifity is not very sensitive to FalsePositives. In the credit default industry a higher FalsePositives is more tolerable than higher FalseNegatives, therefore our score should not be so sensitive to the FalsePositives. \r\n\r\nCombine these two reasons, we chose AUC over F1 score when evaluating our models.\r\n\r\n### 4.3 Baseline Results\r\n\r\nWe used Naive Bayes Classifier, Decision Tree, Random Forest, Logistic regression as our baseline models. We did not use KNN or SVM classifiers because with our limited computing power and large amount of observations and features, both took extremely long to conduct grid search hyperparameter tuning. And they are generally outperformed by logistic regression and random forest according to our experiment. The hyperparameters and test results from our baseline models on each of the four training sets are shown below (see 4.1). From the model results, we discovered that logistic regression with l2 regularization and random forest consistently produce the best ROC_AUC scores on the test sets. To our surprise, the original dataset slightly outperforms the dataset with dimensionality reduction and oversampling.\r\n \r\n\r\nTable 5.  AUC Score of Baseline Models\r\n\r\n<table class=\"tg\">\r\n  <tr>\r\n    <th class=\"tg-0pky\">Dataset</th>\r\n    <th class=\"tg-0pky\">Model</th>\r\n    <th class=\"tg-0lax\">Hyperparameter Tuning Result</th>\r\n    <th class=\"tg-0lax\">ROC_AUC score</th>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\">Original</td>\r\n    <td class=\"tg-0pky\" rowspan=\"4\">Decision Tree</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 6}<br>max_depth=5,<br>min_samples_leaf=1, <br>min_samples_split=2,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.7087</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 1}<br>max_depth=5,<br>min_samples_leaf=4, <br>min_samples_split=2,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.6519</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Original+SMOTE</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 1}<br>max_depth=10,<br>min_samples_leaf=4, <br>min_samples_split=2,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.6635</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA+SMOTE</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 1}<br>max_depth=10,<br>min_samples_leaf=4, <br>min_samples_split=2,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.6313</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Original</td>\r\n    <td class=\"tg-0lax\" rowspan=\"4\">Random Forest</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 6}<br>max_depth=20,<br>min_samples_leaf=1, <br>min_samples_split=2,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.7431</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 6}<br>max_depth=5,<br>min_samples_leaf=1, <br>min_samples_split=6,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.7150</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Original+SMOTE</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 1}<br>max_depth=20,<br>min_samples_leaf=1, <br>min_samples_split=2,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.7125</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA+SMOTE</td>\r\n    <td class=\"tg-0lax\">class_weight={1: 1}<br>max_depth=20,<br>min_samples_leaf=1,<br>min_samples_split=2,<br>n_estimators=100</td>\r\n    <td class=\"tg-0lax\">0.7056</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Original</td>\r\n    <td class=\"tg-0lax\" rowspan=\"4\">Naive Bayes</td>\r\n    <td class=\"tg-0lax\" rowspan=\"4\">N/A</td>\r\n    <td class=\"tg-0lax\">0.5813</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA</td>\r\n    <td class=\"tg-0lax\">0.6474</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Original+SMOTE</td>\r\n    <td class=\"tg-0lax\">0.5359</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA+SMOTE</td>\r\n    <td class=\"tg-0lax\">0.5191</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Original</td>\r\n    <td class=\"tg-0lax\" rowspan=\"4\">Logistic Regression</td>\r\n    <td class=\"tg-0lax\">C=7.5,<br>class_weight={1: 6},<br>fit_intercept=True,<br>penalty='l2',<br>solver='liblinear’</td>\r\n    <td class=\"tg-0lax\">0.7745</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA</td>\r\n    <td class=\"tg-0lax\">C=0.01,<br>class_weight={1: 6}, <br>fit_intercept=True,<br>penalty='l2',<br>solver='liblinear',</td>\r\n    <td class=\"tg-0lax\">0.7594</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Original+SMOTE</td>\r\n    <td class=\"tg-0lax\">C=10.0,<br>class_weight={1: 4},<br>fit_intercept=True,<br>penalty='l2',<br>solver='liblinear'</td>\r\n    <td class=\"tg-0lax\">0.7577</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PCA+SMOTE</td>\r\n    <td class=\"tg-0lax\">C=0.01, <br>class_weight={1: 5}, <br>fit_intercept=True,<br>penalty='l2',<br>solver='liblinear'</td>\r\n    <td class=\"tg-0lax\">0.7463</td>\r\n  </tr>\r\n</table>\r\n\r\n## 5 Experiments on Gradient Boosting Method\r\n\r\n### 5.1 Hyperparameter Tuning by Bayesian Optimization\r\n\r\nGiven our large dataset, we were to use the traditional grid search method to perform hyperparameter tuning, each iteration would take approximately one hour to run. With many combinations of hyperparameters, the process would take way too long to execute. Our group chose to utilize Bayesian Optimization in order to tune hyperparameters for the lightGBM model. Bayesian Optimization is a machine-learning based optimization method designed for objective functions that take a long time to evaluate and is widely used for black-box derivative-free global optimization [6]. In our project, the objective is to find a model with maximum AUC score. So the objective function is:\r\n\r\n\r\n- ![image](images/obj.png)\r\n- f : fitting process of lightGBM \r\n- parameters: model parameters\r\n- X, Y: given training set\r\n\r\n\r\nIn each iteration of the Bayesian Optimization algorithm, it first models the objective function using a Bayesian statistical model, Gaussian Process, to provide posterior probability distributions of the objective function for any given set of parameters. This process is referred to as the “surrogate” for the objective function in literature. The algorithm then uses an acquisition function defined from the surrogate to decide which hyperparameter combination to sample next. The acquisition function is a function of distribution properties (e.g. mean, variance) of the updated posterior distribution we get in the former step. The design of acquisition is a balance between exploration (mean) and exploitation (variance). Large mean means the value is more likely to be the true value of objective function, while large variance means we are more likely to find a larger point on objective function. So by maximizing the acquisition function, we find the ideal hyperparameter sample point for next round. A common acquisition function used is Expected Improvement. By maximizing expected improvement with respect to hyperparameters, the algorithm proposes a new set of hyperparameters for the next iteration. By proposing better hyperparameter combinations for evaluation at each iteration, we explore the value of the objective function in a quicker way than the traditional grid search method [6]. The final hyperparameters we used were the one with the largest AUC score in all iterations.\r\n\r\nWe identified a few shortcomings for this method. Generally, Bayesian Optimization works best when the dimension of parameters to tune is not large. In addition, Bayesian Optimization does not completely guarantee that we would find the global optimum solution, especially when the number of sampling rounds is small or the acquisition function puts more weight on mean instead of variance. Despite the shortcomings, we believe that the efficiency of Bayesian Optimization outweighs its disadvantage. \r\n\r\nFollowing figure shows the settings and results of the Bayesian Optimization in our project. The best model achieved an average AUC score in cross validation as 0.7897, which significantly outperformed the baseline models. So we selected this model as the final model.\r\n\r\n\r\n![image](images/7641%20project%20plot/bayes_opt.png)\r\nFigure 9.  Bayesian Optimization Settings and Results\r\n\r\n\r\n### 5.2 Final Model Evaluation\r\nAfter the hyper-parameter tuning process, we selected the best model and used it as the final model. Thus again, we applied this model on the whole training set (80% of the whole dataset) to train the model. Then, we made predictions on the test set, which contained the 20% remaining samples, so that we can obtain the final evaluation on the performance of the selected model. The AUC score of the model on the test set was 0.7949, which was close to the mean AUC score in cross validation. This implied that our model didn’t suffer from overfitting significantly.\r\n\r\n![image](images/7641%20project%20plot/roc_finalmodel.png)\r\n\r\nFigure 10.  ROC of Final Model on Test Set\r\n\r\n\r\nFurthermore, we output the confusion matrix of the predictions by the selected model. From the matrix we noticed that the recall of our model performed not that well, it had a tendency to miss the real default cases. Combined with the ROC plot, we proposed that a lower probability threshold should be set based on our model.\r\n\r\n![image](images/7641%20project%20plot/cm_finalmodel.png)\r\n\r\nFigure 11.  Confusion Matrix of Final Model on Test Set\r\n\r\n\r\n5.3 Feature Importance and Business Insight\r\nGradient Boosting Model can also provide the evaluation of feature importance according to either the number of times a feature is used in a model to split or the total gains of splits which use a feature. In the following figure, we illustrated the top 20 important features in terms of numbers of times they were used for splitting the nodes.\r\n\r\n\r\n![image](images/7641%20project%20plot/feature_importance.png)\r\n\r\nFigure 12.  Feature Importance from the Final Model\r\n\r\nFrom the results, we can get several groups of factors that are valid for default prediction. The table below illustrates detailed explanations.\r\n\r\n- **Financial pressure of repaying**: in accordance with our daily life experience, how difficult to repay the loan is one of the decisive factors influencing whether or not the client will default. It’s related to the total amount of the credit, how much is the annuity, how long it will take to repay the loan in full and amount and duration of other loans the clients have.\r\n\r\n- **Repaying capability** : the second group of factors are the variables related with the client’s financial capability. The features we got in this project are some indirect indicators that reflect the repaying capability, like client’s age, employment and previous repay behaviors.\r\n\r\n- **History of loan applications**: we also revealed that the duration from previous loan application to current application is also important in default prediction. \r\n\r\nTable 6.  Factor Groups Influencing Default Risk\r\n\r\n<table class=\"tg\">\r\n  <tr>\r\n    <th class=\"tg-0pky\">Factor Group</th>\r\n    <th class=\"tg-0pky\">Variables</th>\r\n    <th class=\"tg-0lax\">Description</th>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0pky\" rowspan=\"6\">Financial pressure of repaying</td>\r\n    <td class=\"tg-0pky\">CREDIT_TERM_APP</td>\r\n    <td class=\"tg-0lax\">How long it will take to repay the loan in full in current application</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">AMT_ANNUITY_APP</td>\r\n    <td class=\"tg-0lax\">The amount of annuity of current application</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">DAYS_CREDIT_ENDDATE_MAX_BUREAU</td>\r\n    <td class=\"tg-0lax\">Max duration of other loans that the applicant has</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">AMT_CREDIT_APP</td>\r\n    <td class=\"tg-0lax\">Total amount of the credit</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">AMT_CREDIT_SUM_DEBT_MEAN_BUREAU</td>\r\n    <td class=\"tg-0lax\">Monthly average debt on Credit Bureau credit</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">AMT_GOODS_PRICE_APP</td>\r\n    <td class=\"tg-0lax\">Monthly average debt on Credit Bureau credit</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\" rowspan=\"7\">Capability of repaying</td>\r\n    <td class=\"tg-0lax\">DAYS_BIRTH_APP</td>\r\n    <td class=\"tg-0lax\">Age of the current applicants</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">DAYS_EMPLOYED_APP</td>\r\n    <td class=\"tg-0lax\">How many days before the application the person started current employment</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">AMT_PAYMENT_SUM_REPAY</td>\r\n    <td class=\"tg-0lax\">Total amount of the client actually paid on previous credit</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">DAYS_EMPLOYED_PERC_APP</td>\r\n    <td class=\"tg-0lax\">The percentage of the days employed relative to the client's age</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PAYMENT_TIME_MAX_REPAY</td>\r\n    <td class=\"tg-0lax\">The time length between installment due date and actual paying date for previous loans</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">OWN_CAR_AGE_APP</td>\r\n    <td class=\"tg-0lax\">The age of the current applicant’s car</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">PAYMENT_GAP_MEAN_REPAY</td>\r\n    <td class=\"tg-0lax\">Mean gap between the amount of installment and the actual amount the client paid of all period</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\" rowspan=\"2\">Previous history of applying loans</td>\r\n    <td class=\"tg-0lax\">DAYS_CREDIT_MAX_BUREAU</td>\r\n    <td class=\"tg-0lax\">Max days before current application did client apply for Credit Bureau credit</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">DAYS_LAST_DUE_1ST_VERSION_MAX_PREVAPP</td>\r\n    <td class=\"tg-0lax\">Relative to application date of current application, max duration of the first due date of the previous application</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\">Others</td>\r\n    <td class=\"tg-0lax\">EXT_SOURCE_1_APP<br>EXT_SOURCE_2_APP<br>EXT_SOURCE_3_APP</td>\r\n    <td class=\"tg-0lax\">Scores from external data source for current loan applications</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\"></td>\r\n    <td class=\"tg-0lax\">DAYS_ID_PUBLISH_APP</td>\r\n    <td class=\"tg-0lax\">How many days before the application did client change the identity document with which he applied</td>\r\n  </tr>\r\n  <tr>\r\n    <td class=\"tg-0lax\"></td>\r\n    <td class=\"tg-0lax\">DAYS_REGISTERATION_APP</td>\r\n    <td class=\"tg-0lax\">How many days before the application did client change his registration</td>\r\n  </tr>\r\n</table>\r\n\r\n## 6 Conclusion\r\n\r\n\r\n### 6.1 Summary\r\n\r\nOur project focused on a real-world business problem: loan default prediction. We applied unsupervised learning method (e.g. K-means) in feature engineering and explored different classification models in default detection. Beyond what we have learned in class, the innovation points of our project are:\r\nWe applied the popular Gradient Boosting approach and as expected, it outperformed the other baseline models we used, like random forest and logistic regression.\r\nWe applied Bayesian Optimization method in hyperparameters tuning for the lightGBM model. It significantly increased the efficiency compared with traditional grid search.\r\n\r\nIn the meanwhile, we identified several groups of factors that are valid for default prediction. They are variables that measure clients’ financial pressure of repaying, that reflect clients’ repaying capability and that record clients’ history of loan applications.\r\n\r\n### 6.2 Future work\r\n\r\nDue to limited time, several things we thought through but didn’t included in current project scope:\r\n\r\n1) In our current project, we only modeled the linear or pairwise linear relations between all the features and the response variable. However, the relationship can be quadratic. We may create polynomial terms and interaction terms for important features.\r\n\r\n2) We didn’t spend much time on feature selection. Involving a large number of features may cause potential overfitting problems since the model has a high level of complexity. In our project, we used PCA to reduce dimensions and explored L1 regularization in logistic regression. If we got more time, we may build lightGBM on selected features. \r\n\r\n## 7 Team Contribution\r\n\r\nAll team members have contributed a similar amount of effort.\r\n\r\n## Reference\r\n\r\n[1] Lemmens, A., & Croux, C. (2006). Bagging and boosting classification trees to predict churn. Journal of Marketing Research, 43(2), 276-286.\r\n\r\n[2] Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232.\r\n\r\n[3] Brown, I., & Mues, C. (2012). An experimental comparison of classification algorithms for imbalanced credit scoring data sets. Expert Systems with Applications, 39(3), 3446-3453.\r\n\r\n[4] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. In Advances in neural information processing systems (pp. 3146-3154).\r\n\r\n[5] Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-sne. Journal of machine learning research, 9, 2579-2605.\r\n\r\n[6] Frazier, P. I. (2018). A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811.\r\n\r\n## Appendix \r\n1. [Code repository](https://github.gatech.edu/sding64/sding64.github.io/tree/master/code_data_repository_7641)\r\n\r\n2. [Data aggregation description](https://github.gatech.edu/sding64/sding64.github.io/blob/master/code_data_repository_7641/Data_aggregation_method_description.txt)\r\n\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}